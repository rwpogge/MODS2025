{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fe174bc-aa86-4c11-a444-94c7ecc28188",
   "metadata": {},
   "source": [
    "# MODS Archon FITS header lab\n",
    "\n",
    "Lab notebook for learning how to deconstruct, fix, and work with azcam/archon-generated FITS headers from the MODS instruments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc82c014-f56b-4537-aec0-959403b0b86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Set up matplotlib\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator, LogLocator, NullFormatter\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# astropy packages we need for FITS, time, and coordinates\n",
    "\n",
    "from astropy.io import fits\n",
    "from astropy.table import Table\n",
    "from astropy import units as u\n",
    "from astropy.coordinates import SkyCoord, EarthLocation, Angle\n",
    "from astropy.time import Time\n",
    "\n",
    "# other bits\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08db4b8a-c3dc-4422-a67a-e0175d88f7cd",
   "metadata": {},
   "source": [
    "## Plot setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab5d54e7-66b6-4f10-ae1d-ab11436aee95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aspect ratio\n",
    "\n",
    "aspect = 2.5\n",
    "\n",
    "#\n",
    "# Don't change these unless you really need to (we never have).  They make for good resolution\n",
    "# and scale for insertion into LaTeX and Word documents\n",
    "#\n",
    "# fPage is the horizontal fraction of the page occupied by the figure, default 1.0\n",
    "#\n",
    "# scaleFac is the LaTeX includegraphics scaling in units of \\textwidth, default 1.0\n",
    "#\n",
    "\n",
    "fPage = 1.0\n",
    "scaleFac = 1.0\n",
    "\n",
    "# Text width in inches - don't change, this is defined by the print layout for most portrait LaTeX templates\n",
    "\n",
    "textWidth = 6.0 # inches\n",
    "\n",
    "# Graphic dimensions depending on bitmap or vector format (draft vs production)\n",
    "\n",
    "figFmt = 'png'\n",
    "dpi = 600\n",
    "plotWidth = dpi*fPage*textWidth\n",
    "plotHeight = plotWidth/aspect\n",
    "axisFontSize = 8\n",
    "labelFontSize = 6\n",
    "lwidth = 0.5\n",
    "axisPad = 5\n",
    "wInches = fPage*textWidth # float(plotWidth)/float(dpi)\n",
    "hInches = wInches/aspect  # float(plotHeight)/float(dpi)\n",
    "\n",
    "# LaTeX is used throughout for markup of symbols, Times-Roman serif font\n",
    "\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('font', **{'family':'serif','serif':['Times-Roman'],'weight':'bold','size':'16'})\n",
    "\n",
    "# Font and line weight defaults for axes\n",
    "\n",
    "matplotlib.rc('axes',linewidth=lwidth)\n",
    "matplotlib.rcParams.update({'font.size':axisFontSize})\n",
    "\n",
    "# axis and label padding\n",
    "\n",
    "plt.rcParams['xtick.major.pad']=f'{axisPad}'\n",
    "plt.rcParams['ytick.major.pad']=f'{axisPad}'\n",
    "plt.rcParams['axes.labelpad'] = f'{axisPad}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4308c0c-c5d0-456b-adb5-491dcf9ce5fa",
   "metadata": {},
   "source": [
    "## MODS CCD Image Basics\n",
    "\n",
    "Read and sniff the headers of a MODS FITS file.  Format is multi-extension FITS with 5 extensions\n",
    " * PRIMARY - primary HDU with the full FITS header created by azcam including instrument and telescope when enabled.\n",
    " * Q1 - Image HDU for CCD quadrant 1\n",
    " * Q2 - Image HDU for CCD quadrant 2\n",
    " * Q3 - Image HDU for CCD quadrant 3\n",
    " * Q4 - Image HDU for CCD quadrant 4\n",
    " * CONPARS - BinTable HDU with the Archon controller status snapshot at start of exposure\n",
    "\n",
    "### CCD Layout in detector coordinates\n",
    "\n",
    "<img src=\"MODS_CCD_Readout.png\" width=\"800\">\n",
    "\n",
    "A full frame is 8288x3088 pixels.  There are 32 columns of overscan in each quadrant, so the total number of pixels in each\n",
    "image file is 8352x3088. The central reference pixel is (4144,1544) for converting to full detector coordinates.\n",
    "\n",
    "The 4 quadrants are labeled Q1 through Q4 as shown.  Each is 4176x1544 pixels (4144 active + 32 overscan columns).  Quadrants 1 and 3\n",
    "are readout toward the left corner amplifiers, quadrants 2 and 4 are readout toward the right corner amplifiers.  \n",
    "\n",
    "A full-frame unbinned image is readout in about 22 seconds with a 0.5 second shutter delay before readout starts.\n",
    "\n",
    "### Subframe MODS\n",
    "\n",
    "Subframe region-of-interest (ROI) readout is done in symmetical windows centered on the reference pixel.  We currently\n",
    "support three subframe ROI modes:\n",
    " * 1024x1024, 512x512 pixels/quadrant\n",
    " * 3088x3088, 1544x1544 pixels/quadrant\n",
    " * 4096x3088, 2048x1544 pixels/quadrant\n",
    "\n",
    "Each has 32 columns of overscan per quadrant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba995b9-0586-485a-a9d3-752eb42b25c4",
   "metadata": {},
   "source": [
    "## Convenience functions\n",
    "\n",
    "### fixDataSec(hdu) - fix DATASEC header info in quadrant HDUs\n",
    "\n",
    "The coordinates in the `DETSEC` and `CCDSEC` keywords for each quadrant and the `LVTn` coordinate transform\n",
    "coefficients are incorrect.  If you read the raw MODS FITS image using ds9 and open as \"Mosaic IRAF\", the\n",
    "quadrants will not be stitched together correctly on the display (huge gaps) and the cursor will read\n",
    "the wrong coordinates back.  This fixes the headers in post-processing.  Eventually we should dig deep\n",
    "into the `azcam` code and fix it there, but not now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50a018b7-af15-4c8e-b364-927a2b04e13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixDataSec(hdu):\n",
    "    # get header parameters we need\n",
    "\n",
    "    c0=int(hdu[0].header['ref-pix1']) # reference pixel \n",
    "    r0=int(hdu[0].header['ref-pix2'])\n",
    "\n",
    "    nc=hdu[1].header['naxis1'] # total size of a quadrant (all same)\n",
    "    nr=hdu[1].header['naxis2']\n",
    "\n",
    "    ncbias = hdu[1].header['ovrscan1'] # bias columns\n",
    "\n",
    "    # compute corrected DETSEC/CCDSEC and LVTn parameters\n",
    "\n",
    "    detsec1=f\"[{c0-nc+ncbias+1}:{c0},{r0-nr+1}:{r0}]\"\n",
    "    detsec2=f\"[{c0+nc+ncbias}:{c0+1},{r0-nr+1}:{r0}]\" # flip x\n",
    "    detsec3=f\"[{c0-nc+ncbias+1}:{c0},{r0+1}:{r0+nr}]\"\n",
    "    detsec4=f\"[{c0+nc+ncbias}:{c0+1},{r0+1}:{r0+nr}]\" # flip x\n",
    "\n",
    "    q24_LTV1 = c0 + nc - ncbias + 1 \n",
    "    q34_LTV2 = -r0\n",
    "\n",
    "    # Rrecompute and correct DETSEC and CCDSEC\n",
    "\n",
    "    hdu[1].header['detsec'] = (detsec1,'Corrected DETSEC')\n",
    "    hdu[2].header['detsec'] = (detsec2,'Corrected DETSEC')\n",
    "    hdu[3].header['detsec'] = (detsec3,'Corrected DETSEC')\n",
    "    hdu[4].header['detsec'] = (detsec4,'Corrected DETSEC')\n",
    "\n",
    "    hdu[1].header['ccdsec'] = (detsec1,'Corrected CCDSEC')\n",
    "    hdu[2].header['ccdsec'] = (detsec2,'Corrected CCDSEC')\n",
    "    hdu[3].header['ccdsec'] = (detsec3,'Corrected CCDSEC')\n",
    "    hdu[4].header['ccdsec'] = (detsec4,'Corrected CCDSEC')\n",
    "\n",
    "    # fix LTVn so the ds9 cursor returns correct physical pixel coords \n",
    "    # in Q2,Q3, and Q4 (Q1 is OK)\n",
    "\n",
    "    hdu[2].header['LTV1'] = (q24_LTV1,'Corrected LTV1')\n",
    "    hdu[4].header['LTV1'] = (q24_LTV1,'Corrected LTV1')\n",
    "\n",
    "    hdu[3].header['LTV2'] = (q34_LTV2,'Corrected LTV2')\n",
    "    hdu[4].header['LTV2'] = (q34_LTV2,'Corrected LTV2')\n",
    "\n",
    "    # all done\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd68a3b8-98cc-4031-8235-68d4393e9e83",
   "metadata": {},
   "source": [
    "### fixArchonTemps(hdu) - fix CCD and Archon temperatures in the Primary HDU\n",
    "\n",
    "Reads the CONPARS extension (5) and extracts the CCD, Base, and Archon Backplane temperatures at\n",
    "start of exposure and puts these into primary HDU header.  This corrects wrong `CCDTEMP` and `BASETEMP`\n",
    "that comes from `azcam`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24c93b80-c0ae-4676-a6fb-ba24ad604b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixArchonTemps(hdu):\n",
    "    # Get the Archon controller status snapshot from HDU 5 (bintable)\n",
    "\n",
    "    t = Table(hdu[5].data)\n",
    "    archonDict = dict(zip(t['Keyword'].tolist(),t['Value'].tolist()))\n",
    "\n",
    "    # Which sensor is which depends on the MODS channel\n",
    "\n",
    "    modsID = hdu[0].header['INSTRUME']\n",
    "    if modsID in ['MODS1B','MODS1R']:\n",
    "        ccdSensor = 'MOD10/TEMPB'\n",
    "        baseSensor = 'MOD10/TEMPA'\n",
    "    else:\n",
    "        ccdSensor = 'MOD10/TEMPA'\n",
    "        baseSensor = 'MOD10/TEMPB'\n",
    "        \n",
    "    # fix CCDTEMP and BASETEMP\n",
    "\n",
    "    try:\n",
    "        ccdTemp = float(archonDict[ccdSensor])\n",
    "    except:\n",
    "        ccdTemp = -999.9 # no-read value\n",
    "    hdu[0].header['CCDTEMP'] = (ccdTemp,'CCD Detector Temperature [deg C]')\n",
    "\n",
    "    try:\n",
    "        baseTemp = float(archonDict[baseSensor])\n",
    "    except:\n",
    "        baseTemp = -999.9 # no-read value\n",
    "    hdu[0].header['BASETEMP'] = (baseTemp,'CCD Mount Base Temperature [deg C]')\n",
    "\n",
    "    # Extract and store the Archon backplane temperature\n",
    "\n",
    "    try:\n",
    "        archonTemp = float(archonDict['BACKPLANE_TEMP'])\n",
    "    except:\n",
    "        archonTemp = -999.9 # no-read value\n",
    "    hdu[0].header['ARCHTEMP'] = (archonTemp,'Archon Controller Backplane Temperature [deg C]')\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620c30bc-dc4b-4595-b19d-35cdae3f12c5",
   "metadata": {},
   "source": [
    "### fixDateTime(hdu) - fix date/time info in the Primary HDU\n",
    "\n",
    "Converts the `azcam` style `DATE-OBS` keyword in to LBT Archive compliant ISO8601 format (`azcam` is old school).\n",
    "\n",
    "Also compute MJD and heliocentric and barycentric JD and add it to the headers as `HJD-OBS` and `BJD-OBS` at start\n",
    "of exposure useful for MODS observations involving precise timing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4a9b29f-8e1c-4060-8b97-c7147d89465f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixDateTime(hdu):\n",
    "\n",
    "    # fix DATE-OBS to make it FITS- and LBT Archive format compliant ISO8601\n",
    "\n",
    "    try:        \n",
    "        dateObs = hdu[0].header[\"DATE-OBS\"]\n",
    "        utcObs = hdu[0].header[\"UTC-OBS\"]\n",
    "        newDateObs = f\"{dateObs}T{utcObs}\"\n",
    "        hdu[0].header[\"DATE-OBS\"] = (newDateObs,\"UTC Date at start of obs\")\n",
    "            \n",
    "        # compute modified Julian Date\n",
    "        \n",
    "        obsTime = Time(newDateObs,format=\"isot\",scale=\"utc\")\n",
    "        hdu[0].header[\"MJD-OBS\"] = (obsTime.mjd,\"Modified JD at start of obs [UTC]\")\n",
    "            \n",
    "    except:\n",
    "        # unlikely...\n",
    "        pass\n",
    "        \n",
    "    # Compute heliocentric and barycentric JD.  HJD is in UTC time scale, but BJD is \n",
    "    # in Barycentric Dynamical Time (TDB).  We need site and target coordinate info.\n",
    "        \n",
    "    try:\n",
    "        telRA = hdu[0].header[\"TELRA\"]\n",
    "        telDec = hdu[0].header[\"TELDEC\"]\n",
    "   \n",
    "        obsSite = EarthLocation.of_site(\"LBT\")\n",
    "        obsTime = Time(newDateObs,format=\"isot\",scale=\"utc\",location=obsSite)\n",
    "        obsCoord = SkyCoord(telRA,telDec,unit=(u.hourangle,u.deg),frame=\"icrs\")\n",
    "            \n",
    "        lttBary = obsTime.light_travel_time(obsCoord,\"barycentric\")\n",
    "        lttHelio = obsTime.light_travel_time(obsCoord,\"heliocentric\")\n",
    "        obsBary = obsTime.tdb + lttBary\n",
    "        obsHelio = obsTime.utc + lttHelio            \n",
    "\n",
    "        hdu[0].header[\"HJD-OBS\"] = (obsHelio.mjd,\"Heliocentric MJD at start of obs [UTC]\")\n",
    "        hdu[0].header[\"BJD-OBS\"] = (obsBary.mjd,\"Barycentric MJD at start of obs [TDB]\")\n",
    "    except:\n",
    "        # might be TCS offline so no TELRA/TELDEC, or header corruption\n",
    "        pass\n",
    "      \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87457d7",
   "metadata": {},
   "source": [
    "### fixMisc(hdu) - fix miscellanous FITS info in the Primary HDU\n",
    "\n",
    "Where everything else we need to fix/tweak/add is done. Many of these are for back-compatibility with\n",
    "the original MODS headers where we need to convert azcam-isms into MODS-isms, or add keywords that\n",
    "are used by the LBTO Archive, pipelines, etc.\n",
    "\n",
    "Not everything from old MODS translates to new MODS (and vis-versa), but we do what we can"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11ba4570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixMisc(hdu):\n",
    "\n",
    "    # DD returns mount alt/az in sexigesimal degrees, LBT data archive is orthodox about\n",
    "    # using decimal degrees, but DD only returns decimal angles in radians.  So, we use \n",
    "    # astropy.coordinates Angle() to make the conversion. Why they can't handle this natively...\n",
    "\n",
    "    try:         \n",
    "        decAlt = Angle(hdu[0].header[\"TELALT\"],unit=u.degree).degree\n",
    "        decAz = Angle(hdu[0].header[\"TELAZ\"],unit=u.degree).degree\n",
    "        hdu[0].header[\"TELALT\"] = (decAlt,\"Telescope Altitude at start of obs [deg]\")\n",
    "        hdu[0].header[\"TELAZ\"] = (decz,\"Telescope Azimuth at start of obs [deg]\")\n",
    "    except:\n",
    "        # unlikely...\n",
    "        pass\n",
    "\n",
    "    # Zenith distance is just 90-altitude, but OK\n",
    "    \n",
    "    try:         \n",
    "        zd = 90.0 - Angle(hdu[0].header[\"TELALT\"],unit=u.degree).degree\n",
    "        hdu[0].header[\"ZD\"] = (zd,\"Zenith distance at start of obs [deg]\")\n",
    "    except:\n",
    "        hdu[0].header[\"ZD\"] = (-99.99,\"Zenith distance at start of obs [deg]\")\n",
    "\n",
    "    # LBTWLINK is returned by the DD as a 1/0 boolean, old MODS translated to Up/Down\n",
    "    \n",
    "    try:\n",
    "        lbtWLink = int(hdu[0].header[\"LBTWLINK\"])\n",
    "        if (lbtWLink == 1):\n",
    "            hdu[0].header[\"LBTWLINK\"] = (\"Up\",\"LBT Weather Station Link State\")\n",
    "        else:\n",
    "            hdu[0].header[\"LBTWLINK\"] = (\"Down\",\"LBT Weather Station Link State\")\n",
    "    except:\n",
    "        hdu[0].header[\"LBTWLINK\"] = (\"Unknown\",\"LBT Weather Station Link State\")\n",
    "\n",
    "    # azcam does not record a global merged CCD ROI, like the old MODS CCDROI keyword, so we  \n",
    "    # compute it.  Header 1 (IM1) has the relevant dimensions plus overscan, 0 has ref pix\n",
    "    \n",
    "    try:\n",
    "        nc = hdu[1].header['NAXIS1']\n",
    "        nr = hdu[1].header['NAXIS2']\n",
    "        overx = hdu[1].header['OVRSCAN1']\n",
    "        xref = hdu[0].header['REF-PIX1']\n",
    "        yref = hdu[0].header['REF-PIX2']\n",
    "        \n",
    "        sc = int(xref - nc + overx +1)\n",
    "        ec = int(xref + nc - overx)\n",
    "        sr = int(yref - nr + 1)\n",
    "        er = int(yref + nr)\n",
    "        hdu[0].header[\"CCDROI\"] = (f\"[{sc}:{ec},{sr}:{er}]\",\"CCD subframe ROI coords\")\n",
    "    except:\n",
    "        # unlikely...\n",
    "        pass\n",
    "\n",
    "    # azcam stores binning factors in the CCDSUM keyword as \"(nx ny)\", convert to old-style\n",
    "    # MODS CCDXBIN and CCDYBIN. Get from header 1 (IM1 extension), all are the same.\n",
    "    \n",
    "    try:\n",
    "        ccdsum = hdu[1].header[\"CCDSUM\"]\n",
    "        bits = ccdsum.split(\" \")\n",
    "        hdu[0].header[\"CCDXBIN\"] = (int(bits[0]),\"CCD X-axis Binning Factor\")\n",
    "        hdu[0].header[\"CCDYBIN\"] = (int(bits[1]),\"CCD Y-axis Binning Factor\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Other stuff goes here\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1ab62a-7651-4ab9-8866-b0ae1a9e2d09",
   "metadata": {},
   "source": [
    "## otmProc(hdu) - subtract overscan bias, trim, and merge quadrants into a single image\n",
    "\n",
    "Computes and substracts the overscan bias for each quadrant, trims the overscan columns, flips the images as needed,\n",
    "and merges them into a single 32-bit floating FITS image array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "025bf385-f0ac-431e-a1f2-d5fdc868335b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def otmProc(hdu,biasColOff=2,biasRowOff=2,sigClip=3):\n",
    "    \n",
    "    # Q2 and Q4 are flipped along rows\n",
    "\n",
    "    flipRows = [False,True,False,True]\n",
    "\n",
    "    # Size of data section w/o column bias\n",
    "\n",
    "    ncQuad = hdu[1].header['naxis1'] - hdu[1].header['ovrscan1']\n",
    "    nrQuad = hdu[1].header['naxis2']\n",
    "\n",
    "    ncImg = 2*ncQuad\n",
    "    nrImg = 2*nrQuad\n",
    "\n",
    "    # create an empty array to contian the merged image\n",
    "    \n",
    "    mergedImg = np.empty((nrImg,ncImg),dtype=np.float32)\n",
    "\n",
    "    # starting numpy pixels of each quadrant\n",
    "\n",
    "    startPix = [(0,0),(0,ncQuad),(nrQuad,0),(nrQuad,ncQuad)]\n",
    "\n",
    "    # process by quadrant\n",
    "\n",
    "    quadBias = []\n",
    "    quadStd = []\n",
    "    for quad in [1,2,3,4]:\n",
    "        quadData = hdu[quad].data.astype(np.float32) # convert to 32-bit float for processing\n",
    "        nc = hdu[quad].header['naxis1'] # number of columns\n",
    "        nr = hdu[quad].header['naxis2'] # number of rows\n",
    "        ncbias = hdu[quad].header['ovrscan1'] # number of overscan columns\n",
    "\n",
    "        # extract bias overscan columns\n",
    "    \n",
    "        biasCols = quadData[biasRowOff:-biasRowOff,nc-ncbias+biasColOff:]\n",
    "        bias1d = np.median(biasCols,axis=1)\n",
    "        medBias = np.median(bias1d)\n",
    "        stdBias = np.std(bias1d)\n",
    "        if (sigClip > 0):\n",
    "            loCut = medBias - sigClip*stdBias\n",
    "            hiCut = medBias + sigClip*stdBias\n",
    "            iClip = np.where((bias1d >= loCut) & (bias1d <= hiCut))\n",
    "            medBias= np.median(bias1d[iClip])\n",
    "            stdBias = np.std(bias1d[iClip])                                    \n",
    "        quadBias.append(medBias)\n",
    "        quadStd.append(stdBias)\n",
    "\n",
    "        # subtract the median overscan bias from the quadrant\n",
    "\n",
    "        imgData = quadData[0:,0:nc-ncbias] - medBias\n",
    "\n",
    "        # is this quadrant flipped? Unflip it\n",
    "\n",
    "        if flipRows[quad-1]:\n",
    "            imgData = np.flip(imgData,axis=1)\n",
    "        \n",
    "        # insert the debiased quadrant into the full merged image array\n",
    "    \n",
    "        sr = startPix[quad-1][0]\n",
    "        er = sr + nr\n",
    "        sc = startPix[quad-1][1]\n",
    "        ec = sc + nc - ncbias\n",
    "    \n",
    "        mergedImg[sr:er,sc:ec] = imgData\n",
    "\n",
    "    # all done return the image\n",
    "\n",
    "    return mergedImg, quadBias, quadStd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c4f573-0a49-4b2d-9c99-2bc6e1be5024",
   "metadata": {},
   "source": [
    "## Open the image, scan and fix the header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bcf7ec-5d5d-40c3-8c5b-cf76cbec362f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inFile = \"MODS1B/mods1b.20251224.0008.fits\"\n",
    "\n",
    "hdu = fits.open(inFile)\n",
    "hdu.info()\n",
    "\n",
    "print(len(hdu[0].header['comment']))\n",
    "\n",
    "# fix header issues\n",
    "\n",
    "fixDataSec(hdu)\n",
    "fixArchonTemps(hdu)\n",
    "fixDateTime(hdu)\n",
    "fixMisc(hdu)\n",
    "\n",
    "# check the fixes\n",
    "\n",
    "print(\"\\nPrimary FITS Header:\\n\")\n",
    "print(repr(hdu[0].header))\n",
    "\n",
    "hdu.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f885a65f-d8d4-495b-a777-34046fa3a353",
   "metadata": {},
   "source": [
    "## Quadrant Images\n",
    "\n",
    "Analysis of coordinates and the bias overscan columns. \n",
    "\n",
    "Some questions to address with this section:\n",
    " * what is the bias level?\n",
    "\n",
    "Range is about 950-1050 ADU between CCDs, but very close within a given CCD.  Target was ~1000 ADU\n",
    "\n",
    " * how clean is the bias?\n",
    "\n",
    "Bias is very flat, but initial/end columns and top/bottom rows can sometimes have high values\n",
    "\n",
    " * How many initial columns to skip?\n",
    "\n",
    "2 starting columns **and** 2 rows start/end avoids very high values\n",
    "\n",
    " * Is the bias flat or is there a trend with row?\n",
    "\n",
    "Outside the offsets above, it is very flat with rows, so subtracting a scalar instead of a vector is indicated.\n",
    "\n",
    "This just plots, no writing of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0beead-01aa-45e4-aa08-af63204e3dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "inFile = \"MODS1B/mods1b.20251224.0008.fits\"\n",
    "\n",
    "# open the file\n",
    "\n",
    "hdu = fits.open(inFile)\n",
    "\n",
    "# plotting options\n",
    "\n",
    "showOverscan = False\n",
    "\n",
    "# Q2 and Q4 are flipped along rows\n",
    "\n",
    "flipRows = [False,True,False,True]\n",
    "\n",
    "# bias column analysis\n",
    "\n",
    "biasColOff = 2 # skip 2 columns at start of biassec\n",
    "biasRowOff = 2 # skip 2 rows at start/end of biassec \n",
    "\n",
    "# reference pixels in x and y\n",
    "\n",
    "xref = int(hdu[0].header[\"ref-pix1\"])\n",
    "yref = int(hdu[0].header[\"ref-pix2\"])\n",
    "\n",
    "print(f\"Reference pixel: {xref},{yref}\")\n",
    "\n",
    "# Size of the merged data w/o overscan columns\n",
    "\n",
    "ncImg = 2*(hdu[1].header['naxis1'] - hdu[1].header['ovrscan1'])\n",
    "nrImg = 2*hdu[1].header['naxis2']\n",
    "\n",
    "print(f\"Output size: {ncImg}x{nrImg}\")\n",
    "fullImg = np.empty((nrImg,ncImg),dtype=np.float32)\n",
    "\n",
    "# quadrant\n",
    "\n",
    "for quad in [1,2,3,4]:\n",
    "    \n",
    "    t0 = time.perf_counter() # start performance timer\n",
    "    \n",
    "    quadData = hdu[quad].data.astype(np.float32) # convert to 32-bit float for processing\n",
    "    \n",
    "    nc = hdu[quad].header['naxis1'] # number of columns\n",
    "    nr = hdu[quad].header['naxis2'] # number of rows\n",
    "    ncbias = hdu[quad].header['ovrscan1'] # number of overscan columns\n",
    "    print(f\"\\nImage Q{quad}:\")\n",
    "    print(f\"  Format: {nc}x{nr}, {ncbias} overscan columns\")\n",
    "    print(f\"  dataType: {quadData.dtype}\")\n",
    "    print(f\"  BIASSEC={hdu[quad].header['biassec']}\")\n",
    "    print(f\"  DATASEC={hdu[quad].header['datasec']}\")\n",
    "    print(f\"  DETSEC={hdu[quad].header['detsec']}\")\n",
    "    print(f\"  CCDSEC={hdu[quad].header['ccdsec']}\")\n",
    "\n",
    "    # extract bias overscan columns\n",
    "    \n",
    "    biasCols = quadData[biasRowOff:-biasRowOff,nc-ncbias+biasColOff:]\n",
    "    #print(f\"  biasCols shape: {biasCols.shape}\")\n",
    "    medBias = np.median(biasCols)\n",
    "    meanBias = np.mean(biasCols)\n",
    "    sigBias = np.std(biasCols)\n",
    "    minBias = np.min(biasCols)\n",
    "    maxBias = np.max(biasCols)\n",
    "    print(f\"  Bias: Median={medBias:.2f} +/- {sigBias:.2f}, Min={minBias:.2f} Max={maxBias:.2f} Mean={meanBias:.2f}\")\n",
    "    bias1d = np.median(biasCols,axis=1)\n",
    "    print(f\"        clipped median: {np.median(bias1d):.2f} +/- {np.std(bias1d):.2f} mean: {np.mean(bias1d):.2f}\")\n",
    "\n",
    "    # subtract bias from the data\n",
    "\n",
    "    imgData = quadData[:,:nc-ncbias-1] - medBias\n",
    "    # print(f\"  imgData data type: {imgData.dtype}\")\n",
    "    \n",
    "    if flipRows[quad-1]:\n",
    "        imgData = np.flip(imgData,axis=1)\n",
    "        print(f\"  flipped along rows\")\n",
    "        \n",
    "    t1 = time.perf_counter()\n",
    "    biasTime = t1 - t0\n",
    "    print(f\"  debias time: {biasTime:.3f} sec\")\n",
    "    \n",
    "    # basic image stats\n",
    "    \n",
    "    imgMed = np.median(imgData)\n",
    "    imgStd = np.std(imgData)\n",
    "    print(f\"   Min: {np.min(imgData):.2f}\")\n",
    "    print(f\"   Max: {np.max(imgData):.2f}\")\n",
    "    print(f\"  Mean: {np.mean(imgData):.2f}, Median: {imgMed:.2f}\")\n",
    "    print(f\"   Std: {imgStd:.2f}\")\n",
    "\n",
    "    t2 = time.perf_counter()\n",
    "    statsTime = t2 - t1\n",
    "    print(f\"  stats time: {statsTime:.3f} sec\")\n",
    "    \n",
    "    # plot bias subtracted image (or whole image)\n",
    "    \n",
    "    fig,(ax1,ax2) = plt.subplots(1,2,figsize=(wInches,hInches),dpi=dpi)\n",
    "    fig.subplots_adjust(wspace=0.3, hspace=0.0)\n",
    "    \n",
    "    if showOverscan:\n",
    "        ax1.imshow(quadData-medBias, cmap=\"gray\",vmin=imgMed-medBias-imgStd,vmax=imgMed-medBias+imgStd)\n",
    "    else:\n",
    "        ax1.imshow(imgData, cmap=\"gray\",vmin=imgMed-imgStd,vmax=imgMed+imgStd)\n",
    "    ax1.tick_params('both',length=2,width=lwidth,which='major',direction='out',top='on',right='on')\n",
    "    ax1.tick_params('both',length=1,width=lwidth,which='minor',direction='out',top='on',right='on')\n",
    "    ax1.set_title(f\"Q{quad} Bias-subtracted\")\n",
    "    \n",
    "    # plot column bias vs. row pixel\n",
    "    \n",
    "    ax2.tick_params('both',length=4,width=lwidth,which='major',direction='in',top='on',right='on')\n",
    "    ax2.tick_params('both',length=2,width=lwidth,which='minor',direction='in',top='on',right='on')\n",
    "    ax2.plot(bias1d,'.',lw=0.5,ms=1,color='black')\n",
    "    ax2.set_xlabel(\"Row pixel\")\n",
    "    ax2.set_ylabel(\"median bias\")\n",
    "    ax2.set_title(f\"Q{quad} Overscan Bias\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "hdu.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b1d503",
   "metadata": {},
   "source": [
    "## Overscan debias, trim, and merge quadrants into as single image\n",
    "\n",
    "Operations:\n",
    " * read in the image\n",
    "   - fix known header issues\n",
    "   - compute full image size, create empty 32-bit float array\n",
    " * for each quadrant:\n",
    "   - compute and subtract overscan bias\n",
    "   - if Q2 or Q4, flip in rows\n",
    "   - add to the full image\n",
    " * add the overscan, trimmed, and merged image to the FITS file\n",
    "   - create a new HDU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0032d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inFile = \"MODS1B/mods1b.20251224.0008.fits\"\n",
    "\n",
    "# open the file\n",
    "     \n",
    "t0 = time.perf_counter() # start performance timer\n",
    "    \n",
    "hdu = fits.open(inFile)\n",
    "\n",
    "# Fix header issues\n",
    "\n",
    "fixDataSec(hdu)\n",
    "fixArchonTemps(hdu)\n",
    "fixDateTime(hdu)\n",
    "\n",
    "# subtract overscan bias, trim, and merge quadrants into a single \"otm\" image\n",
    "\n",
    "otmImg,quadBias,quadStd = otmProc(hdu)\n",
    "\n",
    "# make an image header data unit for the OTM image and append it to the main HDU\n",
    "# with a copy of the full header from the primary HDU plus header cards for\n",
    "# the bias overscan values subtracted as a record of the processing.\n",
    "\n",
    "otmHDU = fits.ImageHDU(data=otmImg,header=hdu[0].header,name=\"Merged\")\n",
    "\n",
    "for quad in [1,2,3,4]:\n",
    "    otmHDU.header[f\"Q{quad}Bias\"] = (quadBias[quad-1],f\"Q{quad} median overscan bias [DN]\")\n",
    "    otmHDU.header[f\"Q{quad}Std\"] = (quadStd[quad-1],f\"Q{quad} overscan bias stdev [DN]\")\n",
    "\n",
    "hdu.append(otmHDU)\n",
    "\n",
    "# write to a new file, old base name with _otm suffix (overscan, trimmed, merged)\n",
    "\n",
    "baseName = os.path.splitext(inFile)[0]\n",
    "newFITS = f\"{baseName}_otm.fits\"\n",
    "hdu.writeto(newFITS,overwrite=True)\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "print(f\"Subtract overscan bias, trimmed, merged, and saved: {(t1-t0):.3f} sec\")\n",
    "\n",
    "hdu.close()\n",
    "\n",
    "# quick stats and display \n",
    "\n",
    "imgMed = np.median(otmImg)\n",
    "imgStd = np.std(otmImg)\n",
    "loCut = imgMed - 3*imgStd\n",
    "hiCut = imgMed + 3*imgStd\n",
    "iClip = np.where((otmImg >= loCut) & (otmImg <= hiCut))\n",
    "imgMed = np.median(otmImg[iClip])\n",
    "imgMean = np.mean(otmImg[iClip])\n",
    "imgStd = np.std(otmImg[iClip])\n",
    "\n",
    "print(f\"Full image: median: {imgMed:.4f} +/- {imgStd:.4f} DN, mean: {imgMean:.4f} DN\")\n",
    "\n",
    "# plot bias subtracted image (or whole image)\n",
    "    \n",
    "fig,ax = plt.subplots(figsize=(wInches,hInches),dpi=dpi)\n",
    "\n",
    "ax.imshow(otmImg, cmap=\"gray\",vmin=imgMed-3*imgStd,vmax=imgMed+3*imgStd)\n",
    "ax.tick_params('both',length=2,width=lwidth,which='major',direction='out',top='on',right='on')\n",
    "ax.tick_params('both',length=1,width=lwidth,which='minor',direction='out',top='on',right='on')\n",
    "ax.set_title(f\"Processed, merged image\",fontsize=labelFontSize)\n",
    "    \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
